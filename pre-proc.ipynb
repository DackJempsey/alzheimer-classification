{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries \n",
    "import torch \n",
    "from PIL import Image \n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "# Read a PIL image \n",
    "image = Image.open('converted_images/FSL_SEG_OAS1_0001_MR1_mpr_n4_anon_111_t88_masked_gfc_fseg_OAS2_126.png') \n",
    "  \n",
    "# Define a transform to convert PIL  \n",
    "# image to a Torch tensor \n",
    "transform = transforms.Compose([ \n",
    "    transforms.PILToTensor() \n",
    "]) \n",
    "  \n",
    "# transform = transforms.PILToTensor() \n",
    "# Convert the PIL image to Torch tensor \n",
    "img_tensor = transform(image) \n",
    "print(0 == torch.tensor(0, dtype=torch.uint8))\n",
    "# print the converted Torch tensor \n",
    "# print(img_tensor)\n",
    "# for i in img_tensor:\n",
    "#     for j in i:\n",
    "#         for k in j:\n",
    "#             if k !=0:\n",
    "#                 print(k)\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan: \n",
    "1.) apply naive ML techniques to the csv data, try to gain insights.\\\\\n",
    "2.) add pre-processing graph layers.\\\\\n",
    "3.) try to infer age and other metrics.\\\\\n",
    "\n",
    "## House keeping:\n",
    "need to properly use 3d spacial data (from gif format)\\\\\n",
    "start with the same cross section of each patient?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as sk_OLS\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n",
      "327\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "# encode M/F, and Hand\n",
    "\n",
    "cross_sectional = pd.read_csv(\"datacsv/oasis_cross-sectional.csv\")\n",
    "cross_sectional.fillna(value=0, inplace=True)\n",
    "y = cross_sectional.CDR\n",
    "X = cross_sectional.drop([\"CDR\", \"M/F\", \"Hand\", \"ID\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "# print(X_test)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(x):\n",
    "    if x == \"Nondemented\":\n",
    "        return 0.0\n",
    "    elif x == \"Demented\" or x == \"Converted\":\n",
    "        return 1.0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ID M/F Hand  Age  Educ  SES  MMSE  CDR  eTIV   nWBV    ASF  \\\n",
      "0    OAS1_0001_MR1   F    R   74   2.0  3.0  29.0  0.0  1344  0.743  1.306   \n",
      "1    OAS1_0002_MR1   F    R   55   4.0  1.0  29.0  0.0  1147  0.810  1.531   \n",
      "2    OAS1_0003_MR1   F    R   73   4.0  3.0  27.0  0.5  1454  0.708  1.207   \n",
      "3    OAS1_0004_MR1   M    R   28   0.0  0.0   0.0  0.0  1588  0.803  1.105   \n",
      "4    OAS1_0005_MR1   M    R   18   0.0  0.0   0.0  0.0  1737  0.848  1.010   \n",
      "..             ...  ..  ...  ...   ...  ...   ...  ...   ...    ...    ...   \n",
      "804              0   M    R   82   0.0  1.0  28.0  0.5  1693  0.694  1.037   \n",
      "805              0   M    R   86   0.0  1.0  26.0  0.5  1688  0.675  1.040   \n",
      "806              0   F    R   61   0.0  2.0  30.0  0.0  1319  0.801  1.331   \n",
      "807              0   F    R   63   0.0  2.0  30.0  0.0  1327  0.796  1.323   \n",
      "808              0   F    R   65   0.0  2.0  30.0  0.0  1333  0.801  1.317   \n",
      "\n",
      "     Delay Subject ID         MRI ID  Group  Visit  MR Delay  EDUC  \n",
      "0      0.0          0              0    0.0    0.0       0.0   0.0  \n",
      "1      0.0          0              0    0.0    0.0       0.0   0.0  \n",
      "2      0.0          0              0    0.0    0.0       0.0   0.0  \n",
      "3      0.0          0              0    0.0    0.0       0.0   0.0  \n",
      "4      0.0          0              0    0.0    0.0       0.0   0.0  \n",
      "..     ...        ...            ...    ...    ...       ...   ...  \n",
      "804    0.0  OAS2_0185  OAS2_0185_MR2    1.0    2.0     842.0  16.0  \n",
      "805    0.0  OAS2_0185  OAS2_0185_MR3    1.0    3.0    2297.0  16.0  \n",
      "806    0.0  OAS2_0186  OAS2_0186_MR1    0.0    1.0       0.0  13.0  \n",
      "807    0.0  OAS2_0186  OAS2_0186_MR2    0.0    2.0     763.0  13.0  \n",
      "808    0.0  OAS2_0186  OAS2_0186_MR3    0.0    3.0    1608.0  13.0  \n",
      "\n",
      "[809 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "longitudinal = pd.read_csv(\"datacsv/oasis_longitudinal.csv\")\n",
    "cross_sectional = pd.read_csv(\"datacsv/oasis_cross-sectional.csv\")\n",
    "\n",
    "frames = [cross_sectional, longitudinal]\n",
    "combined = pd.concat(frames)\n",
    "# combined = combined.rename(columns={\"Group\": \"CDR\"})\n",
    "combined = combined.map(rename)\n",
    "# combined[\"Group\"] = combined[\"Group\"].astype('float')\n",
    "# print(combined)\n",
    "combined.reset_index(inplace=True, drop=True)\n",
    "combined.fillna(value=0, inplace=True)\n",
    "print(combined)\n",
    "combined.to_csv(\"combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n",
      "203\n",
      "     Age  SES  MMSE  eTIV   nWBV    ASF  MR Delay  NewEd\n",
      "0     74  3.0  29.0  1344  0.743  1.306       0.0    2.0\n",
      "1     55  1.0  29.0  1147  0.810  1.531       0.0    4.0\n",
      "2     73  3.0  27.0  1454  0.708  1.207       0.0    4.0\n",
      "3     28  0.0   0.0  1588  0.803  1.105       0.0    0.0\n",
      "4     18  0.0   0.0  1737  0.848  1.010       0.0    0.0\n",
      "..   ...  ...   ...   ...    ...    ...       ...    ...\n",
      "804   82  1.0  28.0  1693  0.694  1.037     842.0   16.0\n",
      "805   86  1.0  26.0  1688  0.675  1.040    2297.0   16.0\n",
      "806   61  2.0  30.0  1319  0.801  1.331       0.0   13.0\n",
      "807   63  2.0  30.0  1327  0.796  1.323     763.0   13.0\n",
      "808   65  2.0  30.0  1333  0.801  1.317    1608.0   13.0\n",
      "\n",
      "[809 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# longitudinal = pd.read_csv(\"datacsv/oasis_longitudinal.csv\")\n",
    "# longitudinal.fillna(value=0, inplace=True)\n",
    "# longitudinal = longitudinal.rename(columns={\"Group\": \"CDR\"})\n",
    "# longitudinal = longitudinal.map(rename)\n",
    "\n",
    "# # print(cross_sectional)\n",
    "# # print(longitudinal)\n",
    "# frames = [cross_sectional, longitudinal]\n",
    "# combined = pd.concat(frames)\n",
    "\n",
    "# print(len(cross_sectional))\n",
    "# print(longitudinal)\n",
    "\n",
    "y = combined.CDR\n",
    "combined[\"NewEd\"] = combined.Educ + combined.EDUC\n",
    "X = combined.drop([\"CDR\", \"M/F\", \"Hand\", \"Subject ID\", \"MRI ID\", \"ID\", \"Group\", \"Visit\", \"Educ\", \"EDUC\", \"Delay\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(len(X_train))\n",
    "# print(y_train)\n",
    "print(len(X_test))\n",
    "print(X)\n",
    "# print(X_test)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.059113300492610835\n",
      "[[3.42970e+04 9.26000e+02 1.12590e+04 8.20958e+05 4.24960e+02 6.62147e+02\n",
      "  1.57896e+05 4.13600e+03]\n",
      " [3.77500e+03 1.20000e+02 1.03900e+03 7.47020e+04 3.53380e+01 5.93290e+01\n",
      "  1.34100e+04 4.43000e+02]\n",
      " [2.54000e+02 7.00000e+00 5.10000e+01 4.30500e+03 2.06300e+00 3.67400e+00\n",
      "  1.34900e+03 3.40000e+01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = MultinomialNB(alpha=1e-3)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(clf.feature_count_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.896551724137931\n",
      "Coef\n",
      "[[-5.20729919e+04 -3.25843307e+03  1.44825232e+04  4.23327006e+03\n",
      "   4.40715190e+02  3.04892390e+02  3.63274893e+03 -6.97735543e+03]\n",
      " [ 4.74278794e+04  3.12276300e+03 -1.13533693e+04 -5.22513021e+03\n",
      "  -4.38441790e+02 -3.38800727e+02 -4.03466845e+03  3.30748983e+03]\n",
      " [ 7.85533738e+03  2.22047636e+02 -1.50928031e+03 -1.02656806e+03\n",
      "  -1.62802752e+01  2.62724188e+01 -6.72579071e+02  1.36124855e+03]]\n",
      "[(0, -52072.99188713831), (2, 14482.523161246887), (7, -6977.355431534111), (3, 4233.270064983301), (6, 3632.748929810703), (1, -3258.433065286154), (4, 440.7151902194444), (5, 304.8923897947879)]\n",
      "Age\n",
      "MMSE\n",
      "Edu\n",
      "eTIV\n",
      "MR Delay\n",
      "SES\n",
      "nWBV\n",
      "ASF\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf_sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-5, random_state=42)\n",
    "clf_sgd.fit(X_train, y_train)\n",
    "y_pred = clf_sgd.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# why are there three of these \n",
    "print(f\"Coef\\n{clf_sgd.coef_}\")\n",
    "\n",
    "coefficients = clf_sgd.coef_\n",
    "# Find the top weighted coefficients\n",
    "top_coefficients = sorted(enumerate(coefficients[0]), key=lambda x: abs(x[1]), reverse=True)[:8]\n",
    "coef_names = [\"Age\", \"SES\", \"MMSE\", \"eTIV\", \"nWBV\", \"ASF\", \"MR Delay\", \"Edu\"]\n",
    "\n",
    "print(top_coefficients)\n",
    "for i in top_coefficients:\n",
    "    print(coef_names[i[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try method with slice 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import imageio.v3 as iio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ID M/F Hand  Age  Educ  SES  MMSE  CDR  eTIV   nWBV    ASF  \\\n",
      "0  OAS1_0001_MR1   F    R   74   2.0  3.0  29.0  0.0  1344  0.743  1.306   \n",
      "\n",
      "   Delay Subject ID MRI ID  Group  Visit  MR Delay  EDUC  \n",
      "0    0.0          0      0    0.0    0.0       0.0   0.0  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TensorDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/dempsey/Desktop/Cornell/ECE-Courses/ECE5414-AML/Final/alzheimer-classification/pre-proc.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dempsey/Desktop/Cornell/ECE-Courses/ECE5414-AML/Final/alzheimer-classification/pre-proc.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(combined\u001b[39m.\u001b[39mloc[combined[\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOAS1_0001_MR1\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dempsey/Desktop/Cornell/ECE-Courses/ECE5414-AML/Final/alzheimer-classification/pre-proc.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# need to correlate images to ids.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dempsey/Desktop/Cornell/ECE-Courses/ECE5414-AML/Final/alzheimer-classification/pre-proc.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m TensorDataset(slice99, y_train) \u001b[39m# shuffles data generates size 32 for you\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dempsey/Desktop/Cornell/ECE-Courses/ECE5414-AML/Final/alzheimer-classification/pre-proc.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m \u001b[39m# use powers of 2 for batch sizes \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dempsey/Desktop/Cornell/ECE-Courses/ECE5414-AML/Final/alzheimer-classification/pre-proc.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TensorDataset' is not defined"
     ]
    }
   ],
   "source": [
    "single_slice = \"converted_images/FSL_SEG_OAS1_0001_MR1_mpr_n4_anon_111_t88_masked_gfc_fseg_OAS2_99.png\"\n",
    "\n",
    "slice99 = []\n",
    "\n",
    "pathlist = Path(\"converted_images\").glob('*99.png')\n",
    "for path in pathlist:\n",
    "     # because path is object not string\n",
    "     path_in_str = str(path)\n",
    "     img = iio.imread(path_in_str)\n",
    "     slice99.append(img)\n",
    "print(combined.loc[combined['ID'] == \"OAS1_0001_MR1\"])\n",
    "# need to correlate images to ids.\n",
    "train_dataset = TensorDataset(slice99, y_train) # shuffles data generates size 32 for you\n",
    "batch_size = 32 # use powers of 2 for batch sizes \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a two-layer neural network using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 32),  # Input layer to hidden layer 784 is the length of a 1d picture\n",
    "    nn.ReLU(),                        # ReLU activation function\n",
    "    nn.Linear(32, 10)  # Hidden layer to output layer there are 10 possible articles of clothing.\n",
    ")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "loss_array = np.zeros(num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X_batch, y_batch in trainloader:\n",
    "        re_shape = X_batch.shape[0]\n",
    "        outputs = model(X_batch.view(re_shape, -1))\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss_array[epoch] += loss.detach().numpy()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "accuracy = []\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in testloader: # calculate accuracy on each batch\n",
    "        re_shape = X_batch.shape[0]\n",
    "        outputs = model(X_batch.view(re_shape, -1))\n",
    "        _, predicted = torch.max(outputs, axis=1)\n",
    "        count +=1\n",
    "        accuracy.append((predicted.numpy() == y_batch.numpy()).sum().item()/ y_batch.shape[0])\n",
    "\n",
    "print(f\"Max Accuracy: {max(accuracy)}\")\n",
    "print(f\" Avg Accuracy: {sum(accuracy) * 100 / count :.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
